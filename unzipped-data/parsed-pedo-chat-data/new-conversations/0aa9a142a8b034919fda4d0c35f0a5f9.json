{
    "id": "0aa9a142a8b034919fda4d0c35f0a5f9",
    "messages": [
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:17): yup, understood",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:17): and then you want a couple more and have a raidz*12",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:17): and then you lose 2 disks",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:17): and then people shout lots ;)",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:18): depends how many disks you feel is safe in a set really",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:18): well, in the 12x combo, I could actually loose 3, but that's also the penalty I've had to pay to get up to 12 the zfs-add-more-raidzs way",
        "4fe573ff41aefb42de56ddf2771295ea (18:18): did raidz2 make it into 6/06?",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:18): how could you lose 3?",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:18): i was talking about if you could extend",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:19): so not a raidz*8 and 2 raidz*2",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:19): oh right, sorry, read that too fast",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:20): well, I guess the short answer is, this rather..different..way of doing things, would work for the moment, and hopefully I'll be talking about adding disks more like 6-12 months down the road, and with some luck, online expansion will be intergrated by then",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:20): It would just be nice to know that this is likely before going ahead and deploying this",
        "63de8a9d2e614f99e6326c6cab36134c (18:21): Is hot spare working in Raidz on update2?",
        "63de8a9d2e614f99e6326c6cab36134c (18:21): or did that not make it in?",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:21): what kind of hardware are you using Gr|ffous ?",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:21): sorry river/postwait, I'm not sure. I'm running a BFU",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:21): in most cases arent you going to need to buy a new enclosure anyway",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:22): so it would make sense to just grow it by adding another 8 disk shelf and raidz",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:22): nah, external array",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:23): ah ive not even thought about how to use it with proper arrays yet :)",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:24): I guess the thing that bothers me, is visualising just where my data is going. I like to know that from a performance standpoint, I have all my data going across my nice 8 drive array. If I end up doing things like [Raidz*8 + Raidz*2].... well, where's is going?",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:24): striped over both which is why id stick to equal size sets",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:25): and sure, zfs will just 'take care of it for me', but this is one of the fundemental things that bothered me about lvm. Sure, my swap is in there somewhere... but is it at the beginning of the disk, or the end? is it on the fast disks, or the slow ones? *shrug*",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:25): which doesn't lend itself nicely to small upgrades :(",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:26): I hear what you're saying, thank-you, you are being most helpful - but do you at least see what I'm trying to say?",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:26): yeah i can see your point",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:27): and indeed for home use with a couple of disks id probably want to grow from say 4 to 8",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:27): I won't be able to sleep at night!!",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:27): but im not sure the risk of growing from 8 to bigger is worth it when you have a proper budget etc ;)",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:27): just tell people they need to buy 8 more",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:27): rather like now in fact.. Must head to bed",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:27): and yeah im wondering how zfs copes with the start of my disk being twice as fast as the end",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:28): i expect it doesnt",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:28): Yeah, 8 might not be the best example for the case I'm trying to make.",
        "495dc924195a6e2676bb11153b05b166 (18:28): pixie__, it really doesn't care.   that's a driver issue",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:28): if its a highend ish array could you resize the luns one at a time",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:29): guess that would have the same effect as adding 2 more disks worth of space",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:29): kind of :)",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:29): g4lt-mordant yeah but with ufs i can put things i want to be quick at the start",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:29): with zfs it goes wherever copy on write sticks it at the time",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:30): still, the advantages of zfs are worth it i guess :)",
        "495dc924195a6e2676bb11153b05b166 (18:31): so basically, you're attempting to enhance performance based on a broken driver, then complaining it won't work with another fs?",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:31): broken driver?",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:31): so just to be clear on this, if you have 2 raidz's in a pool, zfs will attempt to 'raid50' them?",
        "495dc924195a6e2676bb11153b05b166 (18:32): every disk has differetial speeds based on position on platter.  most disk system drivers average the speeds",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:32): ok will file a bug for the sata driver ;)",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:32): Gr|ffous prettymuch",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:33): I do remember reading a blog entry about zfs being very aware of the disk geometry, and using a scheduler specifically designed to minimise head moment relative to the priority of the read/write requests in the disk queue",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:33): which I thought was freaking awesome",
        "8d11e6adaf4f6e17ae029ff039bcb31d (18:33): ive not run into any problems with it yet tbh , its just something thats been nagging away at me :)",
        "d2235432e3c60b3fa00d7deb6a58bcbf (18:33): but that's a slightly different issue"
    ],
    "person_ids": [
        "d2235432e3c60b3fa00d7deb6a58bcbf",
        "8d11e6adaf4f6e17ae029ff039bcb31d",
        "4fe573ff41aefb42de56ddf2771295ea",
        "63de8a9d2e614f99e6326c6cab36134c",
        "495dc924195a6e2676bb11153b05b166"
    ]
}