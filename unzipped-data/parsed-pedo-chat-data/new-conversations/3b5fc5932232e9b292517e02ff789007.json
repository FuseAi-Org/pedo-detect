{
    "id": "3b5fc5932232e9b292517e02ff789007",
    "messages": [
        "69b0d3dfe919a6b860a9fac82de52a7e (03:24): well i was going to do some research of year-based data based on Last-Modified headers, but most pages don't actually serve them",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:24): so that's gone out of the window",
        "6905646aa63b96d6bdfbd256603fa58e (03:26): 69b0d3dfe919a6b860a9fac82de52a7e: have you thought of using the wayback machine?",
        "6905646aa63b96d6bdfbd256603fa58e (03:26): from archive.org?",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:26): how?",
        "6905646aa63b96d6bdfbd256603fa58e (03:27): they make their data available to researchers",
        "6905646aa63b96d6bdfbd256603fa58e (03:27): they have indexes from crawls by alexa that are roughly every 6 months since about 1994",
        "6905646aa63b96d6bdfbd256603fa58e (03:27): if you want to do comparative study based on time, you could use those buckets",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:29): it's not clear to me that their system could support parsing every single file in their index",
        "6905646aa63b96d6bdfbd256603fa58e (03:29): I think it'd only be possible through the alexa web search apis",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:30): yeah, that's not really enough for what i want to do",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:30): (find how element usage varies over time)",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:30): (and class, and id)",
        "6905646aa63b96d6bdfbd256603fa58e (03:30): yeah, you're probably right",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:33): so i scanned about 100,000 documents (not really at random, so this may not be representative)",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:33): about 100,000 of them had no last-modified headers",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:33): about 20000 of them said 2007",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:33): 1 of them said 200 AD",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:34): oh i see, it actually said Tue, 14 Oct 02003 06:53:14 GMT",
        "bc75c2548b71aed454f81ac6897bd078 (03:34): heh. wise guy, eh?",
        "65325d50b2e25aca54bc871b89758c9c (03:34): served from a stone tablet?",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:34): 1 said 2044",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:34): a number said 2099",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:35): and a spanish one said Mon, 26 Jul 2250 05:00:00 GMT",
        "6905646aa63b96d6bdfbd256603fa58e (03:35): maybe we need to define Time5",
        "6905646aa63b96d6bdfbd256603fa58e (03:35): or Calendar5",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:35): there's also a number of files from 1971 to 1994",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:35): which is impressive since the web started in 1990",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:36): but not impossible",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:36): wow some of them aren't even joking",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:36): http://www.ncbi.nlm.nih.gov/entrez/query.fcgi?cmd=Retrieve&amp;db=PubMed&amp;list_uids=4803661&amp;dopt=Abstract",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:36): ^ 1973",
        "6905646aa63b96d6bdfbd256603fa58e (03:37): I suppose that be an accurate LM header then",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:37): looks like all the 1971-1979 dates are from nih.gov",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:38): and this one from 1985 actually redirects to nih.gov heh",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:46): spec says you MUST use GMT",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:46): apparently some people in europe didn't understand what MUST means",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:46): also what kind of date is &quot;Mon, 22 Jan 2007 23:21:22 GMT,Tue, 07 Feb 2006 09:16:47 GMT&quot; ??",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:47): wow, all kinds of random formats are used",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:47): sheesh",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:47): how hard can this be",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:47): &quot;{ts '2007-04-29 03:40:38'},{ts '2007-04-29 03:40:38'}&quot; is NOT a valid Last-Modified date!",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:47): come on people!",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:58): in my sample of 100000 or so files, there were about 1000 unique _formats_",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:58): for the date",
        "6905646aa63b96d6bdfbd256603fa58e (03:59): any valid ones?",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:59): there are only three valid formats per the spec, which would come up as 10 or so the way i counted it",
        "69b0d3dfe919a6b860a9fac82de52a7e (03:59): so that's about 990 invalid ones",
        "6905646aa63b96d6bdfbd256603fa58e (04:00): and you said &quot;so i scanned about 100,000 documents&quot; and &quot;about 100,000 of them had no last-modified headers&quot;",
        "6905646aa63b96d6bdfbd256603fa58e (04:00): I'm guessing one of those is off by an order of magnitude",
        "69b0d3dfe919a6b860a9fac82de52a7e (04:00): actually no",
        "69b0d3dfe919a6b860a9fac82de52a7e (04:00): i was _about_ 100,000 files, and _about_ 100,000 of them had no date",
        "69b0d3dfe919a6b860a9fac82de52a7e (04:00): both numbers to 1sf",
        "6905646aa63b96d6bdfbd256603fa58e (04:00): gotcha",
        "69b0d3dfe919a6b860a9fac82de52a7e (04:01): actual numbers were closer to 140000 and 100000, i think"
    ],
    "person_ids": [
        "69b0d3dfe919a6b860a9fac82de52a7e",
        "6905646aa63b96d6bdfbd256603fa58e",
        "bc75c2548b71aed454f81ac6897bd078",
        "65325d50b2e25aca54bc871b89758c9c"
    ]
}