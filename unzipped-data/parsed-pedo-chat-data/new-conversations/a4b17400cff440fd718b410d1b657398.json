{
    "id": "a4b17400cff440fd718b410d1b657398",
    "messages": [
        "d2d701f83a3410c444a03f8f870acc4f (23:13): to raid-z or not to raid-z, that is the question",
        "d2d701f83a3410c444a03f8f870acc4f (23:16): given two disk arrays with 12 disks each (both capable of hardware raid 5 with 1GB write-back cache), i'm trying to decide if it's better to use HW raid5 and then a zfs stripe (so 5+0), or use it as JBOD with two raid-z groups",
        "d2d701f83a3410c444a03f8f870acc4f (23:17): to get all the protection zfs offers in terms of end-to-end data integrity, i guess the latter is the obvious",
        "7ef291c89ad915978b203d427919cfbb (23:22): raidz wins, because it has much faster writs",
        "7ef291c89ad915978b203d427919cfbb (23:23): 2x raidz with 6 disks each",
        "d2d701f83a3410c444a03f8f870acc4f (23:23): well, that would be 2x raidz per disk array, so 4x raid-z total",
        "d2d701f83a3410c444a03f8f870acc4f (23:23): actually works out to 11 disks each with one hot spare",
        "d2d701f83a3410c444a03f8f870acc4f (23:24): but you're effectively losing one drive per raid-z group",
        "7ef291c89ad915978b203d427919cfbb (23:24): and i would mix it.. 3 disks from each array in each pool",
        "d2d701f83a3410c444a03f8f870acc4f (23:24): i'm trying to maximize available space",
        "7ef291c89ad915978b203d427919cfbb (23:25): yes, but it allows you to live through a disk failure in each group, do  12x disks in a raidz, and loose two drives and your data is gone",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:26): raidz wont be faster than a hardware mirror",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:26): or raid",
        "7ef291c89ad915978b203d427919cfbb (23:27): rodrickbrown, have you benchmarked that?",
        "7ef291c89ad915978b203d427919cfbb (23:27): what happens to hardware raid, when you move gigabytes and exceed its buffer.",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:28): jamesd, you basically create you hardware raid and prove n luns to hosts",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:28): then stripe those luns at the host level with zfs/svm/vxvm etc..",
        "d2d701f83a3410c444a03f8f870acc4f (23:29): rodrickbrown: yeah, the only drawback with that being that the hardware raid could do all kinds of nasty things with the data that zfs wouldn't be able to detect",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:29): actually there isnt even any reason to stripe at the host at that point just concat the luns with the vm",
        "d2d701f83a3410c444a03f8f870acc4f (23:30): and management of the disks is now in two places",
        "7ef291c89ad915978b203d427919cfbb (23:30): rodrickbrown, i'm still debating if raid5 hw, is faster than raidz, raidz has less word to do. especially if you aren't using it for a very specific work load, like random sized data files,  i'm sure an oracle data base with a known access pattern can be tuned for",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:30): Teknix, welcome to enterprise storage",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:30): a software solution will never outperform a hardware one",
        "d2d701f83a3410c444a03f8f870acc4f (23:30): i've been here for awhile, it's just we have zfs now to consider",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:30): atleast from modern storage systems",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:31): Teknix, do it all at the hardware level provide the luns to your host",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:31): then carve them up with zfs",
        "7ef291c89ad915978b203d427919cfbb (23:31): rodrickbrown, i think you are wrong here... a hardware solution uses set block sizes..",
        "d2d701f83a3410c444a03f8f870acc4f (23:32): my arrays let me select stripe size for the raid5 set, and then has basic options for tuning between sequential or random i/o",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:32): jamesd, i dont think so i cant think of any situation where software would be faster",
        "d2d701f83a3410c444a03f8f870acc4f (23:32): they have firmware similar to the 3511",
        "7ef291c89ad915978b203d427919cfbb (23:33): Teknix, yes, but zfs uses the best size block for the actual current data,  so  locking you into one set size, is not a win",
        "d2d701f83a3410c444a03f8f870acc4f (23:33): but i'm allowing zfs to lay down the data on the disk ever how it wants to",
        "d2d701f83a3410c444a03f8f870acc4f (23:34): how is the stripe size defined at the hardware level affecting the OS layer?",
        "d2d701f83a3410c444a03f8f870acc4f (23:34): as far as the OS is concerned, it's just one big disk",
        "7ef291c89ad915978b203d427919cfbb (23:34): rodrickbrown, and what happens if you are writing your data to a raid5 hw, array and you loose power for 4 days... bamm your data is now corrupt because the array standards only requires 3 days of backup power for the buffer, yes it does happen...",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:35): jamesd, what happens with a volcano errupts in your DC ?",
        "d2d701f83a3410c444a03f8f870acc4f (23:35): i didn't bring this up to start a flame war :)",
        "7ef291c89ad915978b203d427919cfbb (23:36): well DC's have lost power in the last 4 years for longer than 3 days...  at least mine was plausible.",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:36): if your storage subsystem is pretty reliable as it should be there is really no reason to put that overhead at the host level when you can do it in hardware",
        "7ef291c89ad915978b203d427919cfbb (23:36): its cheaper to do at the host level than at the hardware level",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:36): just use zfs as a vm i thikn would give the best perf.",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:36): jamesd, at the cost of speed",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:36): and reliability",
        "7ef291c89ad915978b203d427919cfbb (23:37): and when your raid5 hw array looses a drive, 90% of the time  raidz is faster to resync your data.",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:37): never",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:38): jamesd, i'm not taking JBOD here",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:38): we have systems where we can loose 10 - 15 disks",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:38): the OS will never know",
        "d2d701f83a3410c444a03f8f870acc4f (23:38): that's both good and bad, depending",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:38): if you have adequate hot spares in place it shouldnt be a problem",
        "d2d701f83a3410c444a03f8f870acc4f (23:38): the appeal of zfs is end-to-end integrity and management",
        "7ef291c89ad915978b203d427919cfbb (23:39): okay if you are using  less than 100% of the array, zfs only resyncs blocks that it is using.",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:39): well ill leave it as this",
        "7ef291c89ad915978b203d427919cfbb (23:39): hardware always resyncs 100% of the blocks",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:39): you would be really INSANE",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:39): to buy enterprise storage",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:39): not use the hardware based raid features",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:40): and provide each disk to each host",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:40): and use zfs",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:40): any storage consultant would advise you from doing that",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:40): not just zfs but any host level vm",
        "d2d701f83a3410c444a03f8f870acc4f (23:40): yes, i agree it's dumb to buy high-end hardware raid with the intent to use it as dumb JBOD",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:41): James, not to mention when you need to provide Terabytes of data",
        "d2d701f83a3410c444a03f8f870acc4f (23:41): in my case, these arrays were purchased well before ZFS was even remotely viable, and they've been used with UFS and SVM for some time (raid in hw)",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:41): you really think any HOST can do raid efficiently on say 1024 disks?",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:41): lol",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:41): our HDS has about 4 dedicated procs",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:41): doing that alone",
        "d2d701f83a3410c444a03f8f870acc4f (23:41): SVM has been necessary for &gt;2TB LUNs",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:41): 4 dedicated PPC @ 1.2GHZ",
        "7ef291c89ad915978b203d427919cfbb (23:42): you don't buy the high end you now buy the lowend...  thumper looks damm pretty these days, 24TB of raw storage for $33,000",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:42): Tek, its not the lun size really",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:42): more on the disk count",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:42): raid 5 over SVN across 2048 disks == no resources to run any apps",
        "d2d701f83a3410c444a03f8f870acc4f (23:42): yeah",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:42): this is the point i'm trying to make",
        "d2d701f83a3410c444a03f8f870acc4f (23:42): i know what you're saying",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:43): jamesd, no enterprise companies need enterprise solutions",
        "d2d701f83a3410c444a03f8f870acc4f (23:43): so, what is the largest zfs deployment to date, in terms of how many disks?",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:44): i dont think anyone has those numbers but sun",
        "72bac19ca0a2ff15f99326a8f7e02c10 (23:44): There's.... the company I forget that had large amounts of storage in there.",
        "d2d701f83a3410c444a03f8f870acc4f (23:44): and i'm sure it's info they won't disclose",
        "72bac19ca0a2ff15f99326a8f7e02c10 (23:44): sure, it'd help if I remembered the name, but still :)",
        "8d11e6adaf4f6e17ae029ff039bcb31d (23:44): joyent ?",
        "7ef291c89ad915978b203d427919cfbb (23:44): Teknix, tsumame is using a petabyte filesystem housed on zfs but is multiple pools of storage i guess",
        "72bac19ca0a2ff15f99326a8f7e02c10 (23:44): that's the cookie.",
        "d2d701f83a3410c444a03f8f870acc4f (23:45): would be interesting to see how people are configuring their zfs filesystems, with relative size/# disks and the read/write performance they're getting",
        "8d11e6adaf4f6e17ae029ff039bcb31d (23:45): tsubame is a mix of zfs and lustre",
        "8d11e6adaf4f6e17ae029ff039bcb31d (23:45): and traditional san too",
        "7ef291c89ad915978b203d427919cfbb (23:45): <a href=\"http://blogs.sun.com/roller/page/marchamilton?entry=one_petabyte_and_counting\">http://blogs.sun.com/roller/page/marchamilton?entry=one_petabyte_and_counting</a>",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:45): Teknix, they are not using zfs raid on dedicated disks no one would be insane to",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:45): zfs can scale",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:46): but your better off putting the reliability on your backend system",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:46): and using pools (non raid)",
        "d2d701f83a3410c444a03f8f870acc4f (23:46): jamesd: thanks for that link",
        "3f9e24f372de1c18c3993ee5f9f1adcd (23:46): rodrickbrown: transaction processing workloads requiring massive random io capacity aren't the only thing enterprises need. cheap storage for near-line backup and data retention is another workload altogether, and ZFS on a thumper may be a good choice for that - but still as enterpricey as the lottsa-transacitions-per-second OLTP database. =)",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:47): say on your disk subsystem you have 2TB raided mirrored configured with hot spaces, you create say lun size of say 50gigs",
        "d2d701f83a3410c444a03f8f870acc4f (23:47): it would be nice if ZFS could talk to SAN hardware and coordinate with it",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:47): basically allocate n+luns to the host",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:47): put it in a pool",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:47): and your good to go",
        "d2d701f83a3410c444a03f8f870acc4f (23:47): rodrickbrown: yeah, that is the way i'm leaning now",
        "8d11e6adaf4f6e17ae029ff039bcb31d (23:47): joyent details: <a href=\"http://www.scalewithrails.com/downloads/ScaleWithRails-April2006.pdf\">http://www.scalewithrails.com/downloads/ScaleWithRails-April2006.pdf</a>",
        "8d11e6adaf4f6e17ae029ff039bcb31d (23:48): page 48",
        "8d11e6adaf4f6e17ae029ff039bcb31d (23:48): 22TB",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:48): andersmo, i never said capacity was the only thing needed",
        "d2d701f83a3410c444a03f8f870acc4f (23:48): those are some pretty slides",
        "3f9e24f372de1c18c3993ee5f9f1adcd (23:50): rodrickbrown: no, you said &quot;no enterprise companies need enterprise solutions&quot;, which is what I responded to. Cheap storage definitely has it's place in the enterprise as well, but for different applications than the mighty expensive Hitachi and EMC systems. =)",
        "3f9e24f372de1c18c3993ee5f9f1adcd (23:53): I recently needed a test server with some disk capacity, so I scrounged up an old 450, loaded it with disks and installed SXCR. No need to requisition expensive SAN storage and FC ports, and just great for what I needed. =)",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:55): sxcr?",
        "3f9e24f372de1c18c3993ee5f9f1adcd (23:55): solaris express community release",
        "d2d701f83a3410c444a03f8f870acc4f (23:56): the drawback to &quot;22TB with one big zfs pool and LUNs&quot; is that can only be shared out over one server.",
        "d2d701f83a3410c444a03f8f870acc4f (23:56): ZFS needs QFS/SAM-FS capabilities",
        "d2d701f83a3410c444a03f8f870acc4f (23:57): in a digestable form",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:57): san shared file system is nice",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:57): we have since dumped qfs",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:57): moving over to GFS :(",
        "7ef291c89ad915978b203d427919cfbb (23:57): teknix there is nfs not the purfect solution, but works for now",
        "7ef291c89ad915978b203d427919cfbb (23:57): and then there is  iscsi as well",
        "d2d701f83a3410c444a03f8f870acc4f (23:58): yeah, nfsv4 is an improvement, but the purpose of the SAN is lost if you're pushing everything over gigE",
        "8d11e6adaf4f6e17ae029ff039bcb31d (23:58): page 69 is another good slide",
        "daa08b04a71644cbce3e6e7e5aeeb58e (23:58): all require heavy investment in your ip network",
        "d2d701f83a3410c444a03f8f870acc4f (23:58): and when i built my FC san, iSCSI was still too immature",
        "8d11e6adaf4f6e17ae029ff039bcb31d (23:59): <a href=\"http://www.flickr.com/photos/90155880\">http://www.flickr.com/photos/90155880</a>@N00/78283260",
        "daa08b04a71644cbce3e6e7e5aeeb58e (00:00): pixie__, what is this infrastrucuture being used for?",
        "8d11e6adaf4f6e17ae029ff039bcb31d (00:01): <a href=\"http://www.textdrive.com/\">http://www.textdrive.com/</a>",
        "8d11e6adaf4f6e17ae029ff039bcb31d (00:01): + <a href=\"http://www.strongspace.com/\">http://www.strongspace.com/</a>",
        "d2d701f83a3410c444a03f8f870acc4f (00:02): pixie_: are you the now grey haired sysadmin?",
        "8d11e6adaf4f6e17ae029ff039bcb31d (00:03): im nothing to do with it :)",
        "8d11e6adaf4f6e17ae029ff039bcb31d (00:03): just richlowe mentioned them and i dug out hte urls",
        "daa08b04a71644cbce3e6e7e5aeeb58e (00:04): there isnt enough storage vaults online :-)",
        "30660639e3d32008bb35da4584bddcfc (00:04): I want one of those",
        "daa08b04a71644cbce3e6e7e5aeeb58e (00:04): man this space is so cluttered now",
        "8d11e6adaf4f6e17ae029ff039bcb31d (00:04): id love to know what amazon s3 is using",
        "c359f9cde4c4eb3400a3e3c041b0bc53 (00:05): hello. anybody using pkgsrc on solaris express?",
        "c359f9cde4c4eb3400a3e3c041b0bc53 (00:11): nobady? heh",
        "d2d701f83a3410c444a03f8f870acc4f (00:12): i know at least one person at sun has been using pkgsrc",
        "d2d701f83a3410c444a03f8f870acc4f (00:12): do a search for pkgsrc on blogs.sun.com",
        "c359f9cde4c4eb3400a3e3c041b0bc53 (00:12): ok, thanks",
        "e55e49dbdc0b3b324ddec7711f265523 (00:13): aramdune: well I use it, but no with SE",
        "c359f9cde4c4eb3400a3e3c041b0bc53 (00:13): well, I tried it yesterday with sunpro compilers, and today with gcc -- I can;t seem to gett satisfacatory results, for example, now perl doesn't compile, yesterday gtk hasn't compiled and so on"
    ],
    "person_ids": [
        "d2d701f83a3410c444a03f8f870acc4f",
        "7ef291c89ad915978b203d427919cfbb",
        "daa08b04a71644cbce3e6e7e5aeeb58e",
        "72bac19ca0a2ff15f99326a8f7e02c10",
        "8d11e6adaf4f6e17ae029ff039bcb31d",
        "3f9e24f372de1c18c3993ee5f9f1adcd",
        "30660639e3d32008bb35da4584bddcfc",
        "c359f9cde4c4eb3400a3e3c041b0bc53",
        "e55e49dbdc0b3b324ddec7711f265523"
    ]
}